import torch
from tqdm import tqdm
import torch
import argparse
from ouroboros.kv_cache_model import KVCacheModelLade, KVCacheModelSimpleWithGuess
from ouroboros.cache_engine import CacheEngine
from transformers import AutoTokenizer,AutoModelForCausalLM
from ouroboros.models import LlamaForCausalLM

class SpeculativeDecoding:
    def __init__(self,args, draft_model, verify_model, tokenizer, ngram_cache=None) -> None:
        assert draft_model.device == verify_model.device
        assert args.window_size == args.guess_set_size, "We only support same window_size and guess_set_size now. More combinations will be supported in the future."
        self.args = args
        
        self.draft_model_cache = KVCacheModelLade(draft_model, window_size=args.window_size, guess_set_size=args.guess_set_size, lookahead_level=args.lookahead_level, topk=args.topk)
        self.verify_model_cache = KVCacheModelSimpleWithGuess(verify_model, lookahead_level=args.lookahead_level)
        self.tokenizer = tokenizer
        self.eos_token_id = tokenizer.eos_token_id

        if ngram_cache == None:
            self.ngram_cache = CacheEngine(args.lookahead_level, args.guess_set_size)
    
    @staticmethod
    def add_args(parser) -> None:
        parser.add_argument('--lookahead-level', type=int, default=7, help='The level of lookahead decoding.')
        parser.add_argument('--use-guess', type=bool, default=False, help='whether to use guess.')
        parser.add_argument('--guess-set-size', type=int, default=20, help='The size of the guess set for n-gram retrieving. Defaults to 20. Currently, must be equal to window_size.')
        parser.add_argument('--window-size', type=int, default=20, help='The window size used for n-gram generation. Defaults to 20. Currently, must be equal to guess_set_size.')
        parser.add_argument('--gamma', type=int, default=4, help='The lookahead parameter for generation.')
        parser.add_argument('--max-len', type=int, default=512, help='The maximum length of the generated sequence.')
        parser.add_argument('--topk', type=int, default=3, help='')

    # @staticmethod
    # def generate_candidates():

    # @staticmethod
    # def tree_decoding():

    # @staticmethod
    # def evaluate_posterior():

    # @staticmethod
    # def update_inference_inputs():

    @torch.no_grad()
    def example_generate(self, prefix) -> torch.Tensor:
        """
        Performs ouroboros with an approximate model and a target model to generate a sequence of tokens.

        Args:
            prefix (torch.Tensor): The initial sequence of tokens to start the generation from.
            ngram_cache (CacheEngine, optional): A cache engine for storing and retrieving n-gram predictions. Defaults to None, in which case a new cache engine is created.
            max_len (int, optional): The maximum length of the generated sequence. Defaults to 512.
            gamma (int, optional): The lookahead parameter for generation. Defaults to 4.
            window_size (int, optional): The window size used for n-gram generation. Defaults to 20. Currently, must be equal to guess_set_size.
            eos_token_id (int, optional): The token id representing the end-of-sequence token. Defaults to 2. Should be given by tokenizer.eos_token_id.

        Returns:
            torch.Tensor: The generated sequence of tokens, including the initial prefix and any additional tokens generated by the function.
        """

        seq_len = prefix.shape[1]
        T = seq_len + self.args.max_len
        
        assert prefix.shape[0] == 1, "input batch size must be 1"

        guess_size = self.args.lookahead_level - 1
        
        resample_count = 0
        target_sample_count = 0
        accepted_count = 0
        end_pos = None
        
        while prefix.shape[1] < T:
            # q = M_q[prefix + x_0, x_1, .., x_(gamma-2)]
            prefix_len = prefix.shape[1]

            x, out_len, guess = self.draft_model_cache.generate(prefix, self.ngram_cache, self.args.gamma)
            self.verify_model_cache._forward_with_kvcache(x, guess)

            key_tok = int(x[:,-1])

            gen_len = out_len - prefix_len
            
            n = prefix_len + gen_len - 1

            # print(prefix)
            

            for i in range(gen_len):
                j = x[:, prefix_len + i]
                
                t_tok = self.verify_model_cache._prob_history[:, prefix_len + i - 1, :].argmax(dim=-1, keepdim=True).to(j.device)
                if t_tok != j:
                    remaining_target_tok = self.verify_model_cache._prob_history[:, prefix_len + i: out_len, :].argmax(dim=-1) 
                    remaining_approx_tok = x[:, prefix_len + i:]
                    # from IPython import embed; embed()
                    self.draft_model_cache.update_ngram_cache(remaining_approx_tok, remaining_target_tok) # TODO
                    # reject
                    n = prefix_len + i - 1
                    break
                if j == self.eos_token_id:
                    end_pos = prefix_len + i + 1
                accepted_count += 1
            
            # print(f"n : {n}, i : {i}, prefix_len + gamma - 1: {prefix_len + gamma - 1}")
            assert n >= prefix_len - 1, f"n {n}, prefix_len {prefix_len}"
            prefix = x[:, :n + 1]

            self.draft_model_cache.rollback(n+1)

            corr_ngram = [] # ngram corrected by verify_model

            if n < prefix_len + gen_len - 1:
                t = self.verify_model_cache._prob_history[:, n, :].argmax(dim=-1, keepdim=True)
                if t == self.eos_token_id:
                    end_pos = n + 2

                first_tok = int(self.verify_model_cache._prob_history[:, prefix_len + gen_len - 1, :].argmax(dim=-1))
                beg_pos = prefix_len + gen_len
                guess_num = len(guess) // guess_size
                for i in range(guess_num):
                    real_ngram = tuple([first_tok] + self.verify_model_cache._prob_history[0, beg_pos + i * guess_size : beg_pos + i * guess_size + guess_size - 1, :].argmax(dim=-1).tolist())
                    corr_ngram.append(real_ngram)
                if len(corr_ngram) > 0:
                    self.draft_model_cache.update_in_place(key_tok, corr_ngram)


                self.verify_model_cache.rollback(n+1)
                prefix = torch.cat((prefix, t.to(prefix.device)), dim=1)
            else:
                # find the longest guess
                guess = [item for sublist in guess for item in sublist]
                guess_num = len(guess) // guess_size
                first_tok = int(self.verify_model_cache._prob_history[:, n, :].argmax(dim=-1))
                beg_pos = prefix_len + gen_len
                candidate = [first_tok]
                longest_can_len = 1
                candidate_idx = -1
                tmp_end_pos = n + 2 if first_tok == self.eos_token_id else None
                loc_end_pos = None
                for i in range(guess_num):
                    real_ngram = [first_tok] + self.verify_model_cache._prob_history[0, beg_pos + i * guess_size : beg_pos + i * guess_size + guess_size, :].argmax(dim=-1).tolist()
                    corr_ngram.append(tuple(real_ngram[:-1]))
                    pred_ngram = guess[i * guess_size : (i + 1) * guess_size]
                    ml = 0
                    for j in range(guess_size):
                        ml = j
                        if real_ngram[j] == self.eos_token_id:
                            loc_end_pos = j
                        if real_ngram[j] != pred_ngram[j]:
                            break
                    if ml + 1 > longest_can_len:
                        candidate = real_ngram[:ml + 1]
                        longest_can_len = ml + 1
                        candidate_idx = i
                        tmp_end_pos = loc_end_pos
                if tmp_end_pos is not None:
                    end_pos = beg_pos + candidate_idx * guess_size + tmp_end_pos + 1
                candidate = torch.tensor([candidate], device=prefix.device)
                prefix = torch.cat((prefix, candidate), dim=1)
                if len(corr_ngram) > 0:
                    self.draft_model_cache.update_in_place(key_tok, corr_ngram)
                if candidate_idx != -1:
                    self.verify_model_cache.confirm(n+1, beg_pos + candidate_idx * guess_size, candidate.shape[-1] - 1) # cur_len, start_pos, length
                else:
                    self.verify_model_cache.rollback(n+1)
            if end_pos is not None:
                break

        if end_pos is not None:
            prefix = prefix[:, :end_pos]
        return prefix[:, :T]

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    SpeculativeDecoding.add_args(parser)
    args = parser.parse_args()
    small_model = LlamaForCausalLM.from_pretrained("01-ai/Yi-6B", torch_dtype=torch.float16, device_map='cuda',trust_remote_code=True)
    target_model = LlamaForCausalLM.from_pretrained("01-ai/Yi-6B", torch_dtype=torch.float16, device_map='cuda',trust_remote_code=True)

    tokenizer = AutoTokenizer.from_pretrained("01-ai/Yi-6B")

    SDecoding = SpeculativeDecoding(args, small_model, target_model, tokenizer,)

    prompt = "Please summarize the following paragraph. Officers searched properties in the Waterfront Park and Colonsay View areas of the city on Wednesday. Detectives said three firearms, ammunition and a five-figure sum of money were recovered. A 26-year-old man who was arrested and charged appeared at Edinburgh Sheriff Court on Thursday. Summary: "

    input_ids = tokenizer(prompt, return_tensors='pt').to('cuda')['input_ids']

    ouroboros_output = SDecoding.example_generate(input_ids)

    std_output = target_model.generate(input_ids, do_sample=False, min_length=64, max_length=512)

    print(ouroboros_output[:,:64].equal(std_output[:,:64]))

    print(tokenizer.decode(ouroboros_output[0]))